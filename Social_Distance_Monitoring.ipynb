{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgpQJHNSfsK6"
      },
      "outputs": [],
      "source": [
        "# Import Required Libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import face_detection \n",
        "from sklearn.cluster import DBSCAN\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkNO_rSwILnO",
        "outputId": "07b00497-b0d3-4544-c2b9-94b77adc436a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygki7lR_sFvf"
      },
      "outputs": [],
      "source": [
        "# Path to the Working Environment\n",
        "\n",
        "# On a Local Environment => set BASE_PATH  = \"\"\n",
        "BASE_PATH = \"/content/drive/MyDrive/Social-Distancing-and-Face-Mask-Detection-master/\"\n",
        "\n",
        "# Path to Input Video File in the BASE_PATH\n",
        "FILE_PATH = \"/content/drive/MyDrive/Social-Distancing-and-Face-Mask-Detection-master/video1.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA7SidshJHo5",
        "outputId": "8172f2d9-98ff-4dfa-eacc-4a26a75fb49a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
            "  Cloning https://github.com/hukkelas/DSFD-Pytorch-Inference.git to /tmp/pip-req-build-ytvupgph\n",
            "  Running command git clone -q https://github.com/hukkelas/DSFD-Pytorch-Inference.git /tmp/pip-req-build-ytvupgph\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from face-detection==0.2.1) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from face-detection==0.2.1) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face-detection==0.2.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->face-detection==0.2.1) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->face-detection==0.2.1) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->face-detection==0.2.1) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.3.0->face-detection==0.2.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.3.0->face-detection==0.2.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.3.0->face-detection==0.2.1) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.3.0->face-detection==0.2.1) (2.10)\n",
            "Building wheels for collected packages: face-detection\n",
            "  Building wheel for face-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-detection: filename=face_detection-0.2.1-py3-none-any.whl size=29729 sha256=424a9126a1c63783e1ae0afd938fff0283d805920dbac371a9e2987b63abab3b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2rsl7jb4/wheels/11/5d/8c/04ffb7a0ca5427f3e674703ea75ecb16542e94efcc46d6bc1b\n",
            "Successfully built face-detection\n",
            "Installing collected packages: face-detection\n",
            "Successfully installed face-detection-0.2.1\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f386vU9lJMBd",
        "outputId": "504ca210-9a2f-4673-ee2b-13eb273c9458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0+zzzcolab20220513001918)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.21.6)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 100.1 MB 25 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566186 sha256=264250424f9e2eb238120ba9f82a219c5a5d35392ab21586939fdfbb3ce00565\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/81/3c/884bcd5e1c120ff548d57c2ecc9ebf3281c9a6f7c0e7e7947a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YiUC0pLJXJu",
        "outputId": "b6722822-466e-4e0a-a155-e8430604260b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n"
          ]
        }
      ],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "719a33a7f52c42aaaf5032ae05d55bd2",
            "575899a1af7740b2818daf5e16385f04",
            "d8c5a39683994d22a1c447a363f6a94a",
            "9f05aa4732724e3289e9d902a4c8d631",
            "476980e9bbfb4d26adb24be2580647ca",
            "c0e8bc08f00e4910b6519a2ee921bb28",
            "8b8c6eb5ee7443c9a3da7b525cc39926",
            "70b41c75b9924f58a199f83402d0038e",
            "606c5a1cc0c8486081ede06bd5eacecc",
            "12fb0608168249ee8f66c23c8626fc5c",
            "c22daf04338244ceab2f194cf0b7e467"
          ]
        },
        "id": "2xrRSzoT9EBa",
        "outputId": "d8bc0418-a555-4ae3-979f-de01d6e1a450"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://folk.ntnu.no/haakohu/WIDERFace_DSFD_RES152.pth\" to /root/.cache/torch/hub/checkpoints/WIDERFace_DSFD_RES152.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "719a33a7f52c42aaaf5032ae05d55bd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/459M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize a Face Detector \n",
        "# Confidence Threshold can be Adjusted, Greater values would Detect only Clear Faces\n",
        "\n",
        "detector = face_detection.build_detector(\"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv-5woacC0C5"
      },
      "outputs": [],
      "source": [
        "# Load Pretrained Face Mask Classfier (Keras Model)\n",
        "\n",
        "mask_classifier = load_model(\"/content/drive/MyDrive/Social-Distancing-and-Face-Mask-Detection-master/models/ResNet50_Classifier.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O6mF_orCxN4"
      },
      "outputs": [],
      "source": [
        "# Set the Safe Distance in Pixel Units (Minimum Distance Expected to be Maintained between People)\n",
        "# This Parameter would Affect the Results, Adjust according to the Footage captured \n",
        "\n",
        "threshold_distance = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 516,
          "referenced_widgets": [
            "bebc1db6753f4e4487b661566fc742d6"
          ]
        },
        "id": "ejHxuy-9iZwL",
        "outputId": "0a6c1fc8-34f0-4394-d7fd-e7606238003d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Frames :\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bebc1db6753f4e4487b661566fc742d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1375 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Analyze the Video\n",
        "\n",
        "# Load YOLOv3\n",
        "net = cv2.dnn.readNet(BASE_PATH + \"models/yolov3.weights\", BASE_PATH + \"models/yolov3.cfg\")\n",
        "\n",
        "# Load COCO Classes\n",
        "classes = []\n",
        "with open(BASE_PATH+\"models/\"+\"coco.names\", \"r\") as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "    \n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "# Fetch Video Properties\n",
        "cap = cv2.VideoCapture( FILE_PATH )\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "# Create Directory for Storing Results (Make sure it doesn't already exists !)\n",
        "#os.mkdir(BASE_PATH+\"Results\")\n",
        "#os.mkdir(BASE_PATH+\"Results/Extracted_Faces\")\n",
        "#os.mkdir(BASE_PATH+\"Results/Extracted_Persons\")\n",
        "#os.mkdir(BASE_PATH+\"Results/Frames\")\n",
        "\n",
        "# Initialize Output Video Stream\n",
        "out_stream = cv2.VideoWriter(\n",
        "    BASE_PATH+'Results/Output_chaitu_1.mp4',\n",
        "    cv2.VideoWriter_fourcc('X','V','I','D'),\n",
        "    fps,\n",
        "    (int(width),int(height)))\n",
        "\n",
        "print(\"Processing Frames :\")\n",
        "for frame in tqdm.notebook.tqdm(range(int(n_frames))):\n",
        "    \n",
        "    # Capture Frame-by-Frame\n",
        "    ret, img = cap.read()\n",
        "\n",
        "    # Check EOF\n",
        "    if ret == False:\n",
        "        break;\n",
        "\n",
        "    # Get Frame Dimentions\n",
        "    height, width, channels = img.shape\n",
        "\n",
        "    # Detect Objects in the Frame with YOLOv3\n",
        "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outs = net.forward(output_layers)\n",
        "\n",
        "    class_ids = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "    \n",
        "    # Store Detected Objects with Labels, Bounding_Boxes and their Confidences\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > 0.5:\n",
        "                \n",
        "                # Get Center, Height and Width of the Box\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "\n",
        "                # Topleft Co-ordinates\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "\n",
        "    # Initialize empty lists for storing Bounding Boxes of People and their Faces\n",
        "    persons = []\n",
        "    masked_faces = []\n",
        "    unmasked_faces = []\n",
        "\n",
        "    # Work on Detected Persons in the Frame\n",
        "    for i in range(len(boxes)):\n",
        "        if i in indexes:\n",
        "\n",
        "            box = np.array(boxes[i])\n",
        "            box = np.where(box<0,0,box)\n",
        "            (x, y, w, h) = box\n",
        "\n",
        "            label = str(classes[class_ids[i]])\n",
        "\n",
        "            if label=='person':\n",
        "\n",
        "                persons.append([x,y,w,h])\n",
        "                \n",
        "                # Save Image of Cropped Person\n",
        "                cv2.imwrite(BASE_PATH + \"Results/Extracted_Persons/\"+str(frame)\n",
        "                            +\"_\"+str(len(persons))+\".jpg\",\n",
        "                            img[y:y+h,x:x+w])\n",
        "\n",
        "                # Detect Face in the Person\n",
        "                person_rgb = img[y:y+h,x:x+w,::-1]   # Crop & BGR to RGB\n",
        "                detections = detector.detect(person_rgb)\n",
        "\n",
        "                # If a Face is Detected\n",
        "                if detections.shape[0] > 0:\n",
        "\n",
        "                  detection = np.array(detections[0])\n",
        "                  detection = np.where(detection<0,0,detection)\n",
        "\n",
        "                  # Calculating Co-ordinates of the Detected Face\n",
        "                  x1 = x + int(detection[0])\n",
        "                  x2 = x + int(detection[2])\n",
        "                  y1 = y + int(detection[1])\n",
        "                  y2 = y + int(detection[3])\n",
        "\n",
        "                  try :\n",
        "\n",
        "                    # Crop & BGR to RGB\n",
        "                    face_rgb = img[y1:y2,x1:x2,::-1]   \n",
        "\n",
        "                    # Preprocess the Image\n",
        "                    face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "                    face_arr = np.expand_dims(face_arr, axis=0)\n",
        "                    face_arr = preprocess_input(face_arr)\n",
        "\n",
        "                    # Predict if the Face is Masked or Not\n",
        "                    score = mask_classifier.predict(face_arr)\n",
        "\n",
        "                    # Determine and store Results\n",
        "                    if score[0][0]<0.5:\n",
        "                      masked_faces.append([x1,y1,x2,y2])\n",
        "                    else:\n",
        "                      unmasked_faces.append([x1,y1,x2,y2])\n",
        "\n",
        "                    # Save Image of Cropped Face\n",
        "                    cv2.imwrite(BASE_PATH + \"Results/Extracted_Faces/\"+str(frame) +\"_\"+str(len(persons))+\".jpg\",\n",
        "                                img[y1:y2,x1:x2])\n",
        "\n",
        "                  except:\n",
        "                    continue\n",
        "    \n",
        "    # Calculate Coordinates of People Detected and find Clusters using DBSCAN\n",
        "    person_coordinates = []\n",
        "\n",
        "    for p in range(len(persons)):\n",
        "      person_coordinates.append((persons[p][0]+int(persons[p][2]/2),persons[p][1]+int(persons[p][3]/2)))\n",
        "\n",
        "    clustering = DBSCAN(eps=threshold_distance,min_samples=2).fit(person_coordinates)\n",
        "    isSafe = clustering.labels_\n",
        "\n",
        "    # Count \n",
        "    person_count = len(persons)\n",
        "    masked_face_count = len(masked_faces)\n",
        "    unmasked_face_count = len(unmasked_faces)\n",
        "    safe_count = np.sum((isSafe==-1)*1)\n",
        "    unsafe_count = person_count - safe_count\n",
        "\n",
        "    # Show Clusters using Red Lines\n",
        "    arg_sorted = np.argsort(isSafe)\n",
        "\n",
        "    for i in range(1,person_count):\n",
        "\n",
        "      if isSafe[arg_sorted[i]]!=-1 and isSafe[arg_sorted[i]]==isSafe[arg_sorted[i-1]]:\n",
        "        cv2.line(img,person_coordinates[arg_sorted[i]],person_coordinates[arg_sorted[i-1]],(0,0,255),2)\n",
        "\n",
        "    # Put Bounding Boxes on People in the Frame\n",
        "    for p in range(person_count):\n",
        "\n",
        "      a,b,c,d = persons[p]\n",
        "\n",
        "      # Green if Safe, Red if UnSafe\n",
        "      if isSafe[p]==-1:\n",
        "        cv2.rectangle(img, (a, b), (a + c, b + d), (0,255,0), 2)\n",
        "      else:\n",
        "        cv2.rectangle(img, (a, b), (a + c, b + d), (0,0,255), 2)\n",
        "\n",
        "    # Put Bounding Boxes on Faces in the Frame\n",
        "    # Green if Safe, Red if UnSafe\n",
        "    for f in range(masked_face_count):\n",
        "\n",
        "      a,b,c,d = masked_faces[f]\n",
        "      cv2.rectangle(img, (a, b), (c,d), (0,255,0), 2)\n",
        "\n",
        "    for f in range(unmasked_face_count):\n",
        "\n",
        "      a,b,c,d = unmasked_faces[f]\n",
        "      cv2.rectangle(img, (a, b), (c,d), (0,0,255), 2)\n",
        "\n",
        "    # Show Monitoring Status in a Black Box at the Top\n",
        "    cv2.rectangle(img,(0,0),(width,50),(0,0,0),-1)\n",
        "    cv2.rectangle(img,(1,1),(width-1,50),(255,255,255),2)\n",
        "\n",
        "    xpos = 15\n",
        "\n",
        "    string = \"Total People = \"+str(person_count)\n",
        "    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
        "    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n",
        "\n",
        "    string = \" ( \"+str(safe_count) + \" Safe \"\n",
        "    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
        "    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n",
        "\n",
        "    string = str(unsafe_count)+ \" Unsafe ) \"\n",
        "    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
        "    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n",
        "    \n",
        "    string = \"( \" +str(masked_face_count)+\" Masked \"+str(unmasked_face_count)+\" Unmasked \"+\\\n",
        "             str(person_count-masked_face_count-unmasked_face_count)+\" Unknown )\"\n",
        "    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,255),2)\n",
        "\n",
        "    # Write Frame to the Output File\n",
        "    out_stream.write(img)\n",
        "\n",
        "    # Save the Frame in frame_no.png format\n",
        "    cv2.imwrite(BASE_PATH+\"Results/Frames/\"+str(frame)+\".jpg\",img)\n",
        "\n",
        "    # Exit on Pressing Q Key\n",
        "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release Streams\n",
        "out_stream.release()\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Completed!\n",
        "print(\"Done !\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Social_Distance_Monitoring.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12fb0608168249ee8f66c23c8626fc5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "476980e9bbfb4d26adb24be2580647ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "575899a1af7740b2818daf5e16385f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e8bc08f00e4910b6519a2ee921bb28",
            "placeholder": "​",
            "style": "IPY_MODEL_8b8c6eb5ee7443c9a3da7b525cc39926",
            "value": "100%"
          }
        },
        "606c5a1cc0c8486081ede06bd5eacecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70b41c75b9924f58a199f83402d0038e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "719a33a7f52c42aaaf5032ae05d55bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_575899a1af7740b2818daf5e16385f04",
              "IPY_MODEL_d8c5a39683994d22a1c447a363f6a94a",
              "IPY_MODEL_9f05aa4732724e3289e9d902a4c8d631"
            ],
            "layout": "IPY_MODEL_476980e9bbfb4d26adb24be2580647ca"
          }
        },
        "8b8c6eb5ee7443c9a3da7b525cc39926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f05aa4732724e3289e9d902a4c8d631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12fb0608168249ee8f66c23c8626fc5c",
            "placeholder": "​",
            "style": "IPY_MODEL_c22daf04338244ceab2f194cf0b7e467",
            "value": " 459M/459M [00:48&lt;00:00, 10.1MB/s]"
          }
        },
        "c0e8bc08f00e4910b6519a2ee921bb28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c22daf04338244ceab2f194cf0b7e467": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8c5a39683994d22a1c447a363f6a94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70b41c75b9924f58a199f83402d0038e",
            "max": 481004605,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_606c5a1cc0c8486081ede06bd5eacecc",
            "value": 481004605
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}